#![allow(unused)]
//! OpenAI GPT provider implementation

use super::{ClassificationResult, provider::*};
use anyhow::{Context, Result};
use async_trait::async_trait;
use reqwest::Client;
use serde::{Deserialize, Serialize};
use std::time::Duration;

/// OpenAI provider for GPT models
pub struct OpenAIProvider {
    client: Client,
    config: OpenAIConfig,
    model_tier: ModelTier,
}

impl OpenAIProvider {
    pub fn new(config: OpenAIConfig) -> Result<Self> {
        let client = Client::builder().timeout(Duration::from_secs(config.timeout_secs)).build()?;

        let model_tier = match config.model.as_str() {
            "gpt-3.5-turbo" => ModelTier::Small,
            "gpt-4" | "gpt-4-turbo-preview" => ModelTier::Medium,
            "gpt-4-32k" | "gpt-4-turbo" => ModelTier::Large,
            _ => ModelTier::Medium,
        };

        Ok(Self { client, config, model_tier })
    }

    async fn chat_completion(&self, messages: Vec<ChatMessage>) -> Result<String> {
        let api_key =
            std::env::var(&self.config.api_key_env).context("OpenAI API key not found")?;

        let request = ChatCompletionRequest {
            model: self.config.model.clone(),
            messages,
            temperature: self.config.temperature,
            max_tokens: self.config.max_tokens,
        };

        let response = self
            .client
            .post("https://api.openai.com/v1/chat/completions")
            .header("Authorization", format!("Bearer {}", api_key))
            .json(&request)
            .send()
            .await
            .context("Failed to send request to OpenAI")?;

        if !response.status().is_success() {
            let error_text = response.text().await?;
            return Err(anyhow::anyhow!("OpenAI API error: {}", error_text));
        }

        let result: ChatCompletionResponse =
            response.json().await.context("Failed to parse OpenAI response")?;

        result
            .choices
            .first()
            .and_then(|c| Some(c.message.content.clone()))
            .ok_or_else(|| anyhow::anyhow!("No response from OpenAI"))
    }
}

#[async_trait]
impl LLMProvider for OpenAIProvider {
    fn name(&self) -> &str {
        "openai"
    }

    fn tier(&self) -> ModelTier {
        self.model_tier
    }

    async fn is_available(&self) -> bool {
        std::env::var(&self.config.api_key_env).is_ok()
    }

    fn cost_per_1k_tokens(&self) -> f32 {
        match self.config.model.as_str() {
            "gpt-3.5-turbo" => 0.002,
            "gpt-4" => 0.03,
            "gpt-4-turbo-preview" => 0.01,
            _ => 0.01,
        }
    }

    async fn generate_code(
        &self,
        spec: &Specification,
        context: &ProjectContext,
        options: &GenerationOptions,
    ) -> Result<GeneratedCode> {
        let system_prompt = format!(
            "You are an expert {} developer. Generate clean, well-documented code \
             that follows best practices and existing project patterns.",
            context.language
        );

        let user_prompt = format!(
            "Project context:\n\
             - Language: {}\n\
             - Framework: {:?}\n\
             - Existing patterns: {:?}\n\n\
             Specification:\n{}\n\n\
             Requirements:\n{}\n\n\
             Generate implementation code.",
            context.language,
            context.framework,
            context.patterns,
            spec.content,
            spec.requirements.join("\n")
        );

        let messages = vec![
            ChatMessage { role: "system".to_string(), content: system_prompt },
            ChatMessage { role: "user".to_string(), content: user_prompt },
        ];

        let response = self.chat_completion(messages).await?;

        // Parse code blocks from response
        let files = extract_code_files(&response);

        Ok(GeneratedCode {
            files,
            explanation: "Generated by OpenAI GPT".to_string(),
            confidence: 0.85,
            tokens_used: response.len() / 4, // Rough estimate
            model_used: self.config.model.clone(),
            cached: false,
        })
    }

    async fn explain_implementation(
        &self,
        code: &str,
        spec: &Specification,
    ) -> Result<Explanation> {
        let prompt = format!(
            "Explain how this code implements the specification:\n\n\
             Code:\n{}\n\n\
             Specification:\n{}",
            code, spec.content
        );

        let messages = vec![ChatMessage { role: "user".to_string(), content: prompt }];

        let response = self.chat_completion(messages).await?;

        Ok(Explanation {
            summary: response.clone(),
            details: vec![],
            design_decisions: vec![],
            trade_offs: vec![],
        })
    }

    async fn review_code(&self, code: &str, requirements: &[Requirement]) -> Result<ReviewResult> {
        let req_list = requirements
            .iter()
            .map(|r| format!("- {}: {}", r.id, r.description))
            .collect::<Vec<_>>()
            .join("\n");

        let prompt = format!(
            "Review this code against the requirements:\n\n\
             Code:\n{}\n\n\
             Requirements:\n{}",
            code, req_list
        );

        let messages = vec![ChatMessage { role: "user".to_string(), content: prompt }];

        let response = self.chat_completion(messages).await?;

        // Simple parsing - in production would be more sophisticated
        let meets_requirements = !response.to_lowercase().contains("not met")
            && !response.to_lowercase().contains("missing");

        Ok(ReviewResult {
            issues: vec![],
            suggestions: vec![response],
            meets_requirements,
            confidence: 0.8,
        })
    }

    async fn answer_question(&self, question: &str) -> Result<Option<String>> {
        let messages =
            vec![ChatMessage { role: "user".to_string(), content: question.to_string() }];

        let response = self.chat_completion(messages).await?;
        Ok(Some(response))
    }

    async fn classify_content(&self, content: &str) -> Result<ClassificationResult> {
        let prompt = format!(
            "Classify this content. Return JSON: \
             {{\"is_code\": bool, \"is_doc\": bool, \"is_test\": bool, \
              \"is_config\": bool, \"language\": \"name or null\"}}\n\n\
             Content:\n{}",
            &content[..content.len().min(500)]
        );

        let messages = vec![ChatMessage { role: "user".to_string(), content: prompt }];

        let response = self.chat_completion(messages).await?;

        // Try to parse JSON
        if let Ok(parsed) = serde_json::from_str::<serde_json::Value>(&response) {
            return Ok(ClassificationResult {
                is_code: parsed["is_code"].as_bool().unwrap_or(false),
                is_documentation: parsed["is_doc"].as_bool().unwrap_or(false),
                is_test: parsed["is_test"].as_bool().unwrap_or(false),
                is_config: parsed["is_config"].as_bool().unwrap_or(false),
                language: parsed["language"].as_str().map(String::from),
                confidence: 0.9,
            });
        }

        // Fallback
        Ok(ClassificationResult {
            is_code: false,
            is_documentation: false,
            is_test: false,
            is_config: false,
            language: None,
            confidence: 0.1,
        })
    }

    async fn assess_complexity(&self, task: &str) -> Result<TaskComplexity> {
        let prompt = format!(
            "Assess the complexity of this task on a scale: \
             simple (can be done by a tiny model), \
             moderate (needs a small model), \
             complex (needs a medium model), \
             very complex (needs a large model).\n\n\
             Task: {}",
            task
        );

        let messages = vec![ChatMessage { role: "user".to_string(), content: prompt }];

        let response = self.chat_completion(messages).await?;
        let lower = response.to_lowercase();

        let tier = if lower.contains("simple") {
            ModelTier::Tiny
        } else if lower.contains("moderate") {
            ModelTier::Small
        } else if lower.contains("very complex") {
            ModelTier::Large
        } else {
            ModelTier::Medium
        };

        Ok(TaskComplexity {
            tier,
            reasoning: response,
            estimated_tokens: task.len() / 4 * 2,
            confidence: 0.85,
        })
    }
}

/// OpenAI configuration
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct OpenAIConfig {
    pub api_key_env: String,
    pub model: String,
    pub max_tokens: usize,
    pub temperature: f32,
    pub timeout_secs: u64,
}

impl Default for OpenAIConfig {
    fn default() -> Self {
        Self {
            api_key_env: "OPENAI_API_KEY".to_string(),
            model: "gpt-4-turbo-preview".to_string(),
            max_tokens: 4096,
            temperature: 0.2,
            timeout_secs: 60,
        }
    }
}

/// Chat message structure
#[derive(Debug, Serialize, Deserialize)]
pub struct ChatMessage {
    pub role: String,
    pub content: String,
}

/// OpenAI API request
#[derive(Debug, Serialize)]
pub struct ChatCompletionRequest {
    pub model: String,
    pub messages: Vec<ChatMessage>,
    pub temperature: f32,
    pub max_tokens: usize,
}

/// OpenAI API response
#[derive(Debug, Deserialize)]
pub struct ChatCompletionResponse {
    pub choices: Vec<Choice>,
}

#[derive(Debug, Deserialize)]
pub struct Choice {
    pub message: ChatMessage,
}

/// Extract code files from response
pub fn extract_code_files(response: &str) -> Vec<GeneratedFile> {
    let mut files = Vec::new();
    let mut in_code_block = false;
    let mut current_code = String::new();
    let mut current_lang = String::new();
    let mut current_path = None;

    for line in response.lines() {
        if line.starts_with("```") {
            if in_code_block {
                // End of code block
                if !current_code.is_empty() {
                    files.push(GeneratedFile {
                        path: current_path.unwrap_or_else(|| "generated.txt".to_string()),
                        content: current_code.clone(),
                        language: current_lang.clone(),
                        is_test: current_code.contains("#[test]")
                            || current_code.contains("describe("),
                    });
                }
                current_code.clear();
                current_lang.clear();
                current_path = None;
                in_code_block = false;
            } else {
                // Start of code block
                in_code_block = true;
                let lang = line.trim_start_matches("```").trim();
                current_lang = lang.to_string();
            }
        } else if in_code_block {
            // Check for file path comment
            if line.starts_with("//") || line.starts_with("#") {
                if line.contains("filepath:") || line.contains("file:") {
                    let path = line.split(':').nth(1).map(|s| s.trim().to_string());
                    current_path = path;
                    continue;
                }
            }
            current_code.push_str(line);
            current_code.push('\n');
        }
    }

    // Handle unclosed code block
    if in_code_block && !current_code.is_empty() {
        files.push(GeneratedFile {
            path: current_path.unwrap_or_else(|| "generated.txt".to_string()),
            content: current_code,
            language: current_lang,
            is_test: false,
        });
    }

    files
}
